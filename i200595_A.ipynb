{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Loading and Preprocessing Data**"
      ],
      "metadata": {
        "id": "H2wig1ghZ6db"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffnnd62aXXXb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "def load_data(file_path):\n",
        "    df_credit = pd.read_csv(file_path)\n",
        "    scaler = StandardScaler()\n",
        "    df_credit['Amount'] = scaler.fit_transform(df_credit[['Amount']])\n",
        "    df_credit.drop(['Time'], axis=1, inplace=True)\n",
        "    return df_credit\n",
        "\n",
        "data_path_credit = 'credit_card.csv'\n",
        "df_credit = load_data(data_path_credit)\n",
        "X_train_credit, X_test_credit = train_test_split(df_credit, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a Masked Dataset**"
      ],
      "metadata": {
        "id": "XPViN4-MaBCC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MTElUSdXXXc"
      },
      "outputs": [],
      "source": [
        "def geometric_mask(input_data, mask_rate=0.1):\n",
        "    mask = np.random.geometric(p=mask_rate, size=input_data.shape) <= 1\n",
        "    return input_data * mask\n",
        "\n",
        "X_train_masked_custom = geometric_mask(X_train_credit.values)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building the Autoencoder Model**"
      ],
      "metadata": {
        "id": "j6QAKBfHaExQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_20SCmgXXXd"
      },
      "outputs": [],
      "source": [
        "\n",
        "def custom_contrastive_loss(y_true_custom, y_pred_custom, margin_custom=1):\n",
        "    return tf.reduce_mean(y_true_custom * tf.square(y_pred_custom) + (1 - y_true_custom) * tf.square(tf.maximum(margin_custom - y_pred_custom, 0)))\n",
        "\n",
        "def custom_build_autoencoder(input_dim_custom):\n",
        "    inputs_custom = Input(shape=(input_dim_custom,))\n",
        "    encoded_custom = Dense(64, activation='relu')(inputs_custom)\n",
        "    decoded_custom = Dense(input_dim_custom, activation='sigmoid')(encoded_custom)\n",
        "    model_custom = Model(inputs=inputs_custom, outputs=decoded_custom)\n",
        "    model_custom.compile(optimizer='adam', loss=custom_contrastive_loss)\n",
        "    return model_custom\n",
        "\n",
        "def custom_build_discriminator(input_dim_custom):\n",
        "    inputs_custom = Input(shape=(input_dim_custom,))\n",
        "    x_custom = Dense(64, activation='relu')(inputs_custom)\n",
        "    x_custom = Dense(64, activation='relu')(x_custom)\n",
        "    outputs_custom = Dense(1, activation='sigmoid')(x_custom)\n",
        "    model_custom = Model(inputs=inputs_custom, outputs=outputs_custom)\n",
        "    model_custom.compile(optimizer='\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building the Discriminator Model**"
      ],
      "metadata": {
        "id": "YBircR9VaIid"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-dLc2z3XXXd",
        "outputId": "ecb166ee-cbaa-4899-dc51-47aa3c89275a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Batch 1, Loss: 0.36111724376678467\n",
            "Epoch 1, Batch 2, Loss: 0.355888694524765\n",
            "Epoch 1, Batch 3, Loss: 0.35830187797546387\n",
            "Epoch 1, Batch 4, Loss: 0.3594941794872284\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x000001F8CEF62200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1, Batch 5, Loss: 0.3566878139972687\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x000001F8CEF62200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1, Batch 6, Loss: 0.3573008179664612\n",
            "Epoch 1, Batch 7, Loss: 0.3553857207298279\n",
            "Epoch 1, Batch 8, Loss: 0.35467275977134705\n",
            "Epoch 1, Batch 9, Loss: 0.35301852226257324\n",
            "Epoch 1, Batch 10, Loss: 0.3521229326725006\n",
            "Epoch 1, Batch 11, Loss: 0.350278377532959\n",
            "Epoch 1, Batch 12, Loss: 0.3494091033935547\n",
            "Epoch 1, Batch 13, Loss: 0.34853890538215637\n",
            "Epoch 1, Batch 14, Loss: 0.34775251150131226\n",
            "Epoch 1, Batch 15, Loss: 0.3462434709072113\n",
            "Epoch 1, Batch 16, Loss: 0.3448834717273712\n",
            "Epoch 1, Batch 17, Loss: 0.34428098797798157\n",
            "Epoch 1, Batch 18, Loss: 0.34304279088974\n",
            "Epoch 1, Batch 19, Loss: 0.34210482239723206\n",
            "Epoch 1, Batch 20, Loss: 0.3409128487110138\n",
            "Epoch 1, Batch 21, Loss: 0.3406008780002594\n",
            "Epoch 1, Batch 22, Loss: 0.3397917151451111\n",
            "Epoch 1, Batch 23, Loss: 0.3391832113265991\n",
            "Epoch 1, Batch 24, Loss: 0.33857789635658264\n",
            "Epoch 1, Batch 25, Loss: 0.3378087282180786\n",
            "Epoch 1, Batch 26, Loss: 0.3371098041534424\n",
            "Epoch 1, Batch 27, Loss: 0.3361457884311676\n",
            "Epoch 1, Batch 28, Loss: 0.33525580167770386\n",
            "Epoch 1, Batch 29, Loss: 0.33417370915412903\n",
            "Epoch 1, Batch 30, Loss: 0.33341720700263977\n",
            "Epoch 1, Batch 31, Loss: 0.33205288648605347\n",
            "Epoch 1, Batch 32, Loss: 0.3323639929294586\n",
            "Epoch 1, Batch 33, Loss: 0.33186861872673035\n",
            "Epoch 1, Batch 34, Loss: 0.33098459243774414\n",
            "Epoch 1, Batch 35, Loss: 0.33017322421073914\n",
            "Epoch 1, Batch 36, Loss: 0.32891684770584106\n",
            "Epoch 1, Batch 37, Loss: 0.32825154066085815\n",
            "Epoch 1, Batch 38, Loss: 0.3272292912006378\n",
            "Epoch 1, Batch 39, Loss: 0.32612162828445435\n",
            "Epoch 1, Batch 40, Loss: 0.32531362771987915\n",
            "Epoch 1, Batch 41, Loss: 0.3242890536785126\n",
            "Epoch 1, Batch 42, Loss: 0.32365986704826355\n",
            "Epoch 1, Batch 43, Loss: 0.3225482702255249\n",
            "Epoch 1, Batch 44, Loss: 0.3213360905647278\n",
            "Epoch 1, Batch 45, Loss: 0.32042738795280457\n",
            "Epoch 2, Batch 1, Loss: 0.31935372948646545\n",
            "Epoch 2, Batch 2, Loss: 0.3182956576347351\n",
            "Epoch 2, Batch 3, Loss: 0.31728100776672363\n",
            "Epoch 2, Batch 4, Loss: 0.3160577714443207\n",
            "Epoch 2, Batch 5, Loss: 0.3152744174003601\n",
            "Epoch 2, Batch 6, Loss: 0.31449106335639954\n",
            "Epoch 2, Batch 7, Loss: 0.31344303488731384\n",
            "Epoch 2, Batch 8, Loss: 0.3133232593536377\n",
            "Epoch 2, Batch 9, Loss: 0.31242018938064575\n",
            "Epoch 2, Batch 10, Loss: 0.3111882209777832\n",
            "Epoch 2, Batch 11, Loss: 0.30996760725975037\n",
            "Epoch 2, Batch 12, Loss: 0.30901068449020386\n",
            "Epoch 2, Batch 13, Loss: 0.3081754148006439\n",
            "Epoch 2, Batch 14, Loss: 0.30720898509025574\n",
            "Epoch 2, Batch 15, Loss: 0.3062383234500885\n",
            "Epoch 2, Batch 16, Loss: 0.30516576766967773\n",
            "Epoch 2, Batch 17, Loss: 0.30452343821525574\n",
            "Epoch 2, Batch 18, Loss: 0.303444504737854\n",
            "Epoch 2, Batch 19, Loss: 0.30237773060798645\n",
            "Epoch 2, Batch 20, Loss: 0.3013823926448822\n",
            "Epoch 2, Batch 21, Loss: 0.3004383444786072\n",
            "Epoch 2, Batch 22, Loss: 0.299533873796463\n",
            "Epoch 2, Batch 23, Loss: 0.2985214293003082\n",
            "Epoch 2, Batch 24, Loss: 0.297442227602005\n",
            "Epoch 2, Batch 25, Loss: 0.29639869928359985\n",
            "Epoch 2, Batch 26, Loss: 0.2953416705131531\n",
            "Epoch 2, Batch 27, Loss: 0.29435479640960693\n",
            "Epoch 2, Batch 28, Loss: 0.29329365491867065\n",
            "Epoch 2, Batch 29, Loss: 0.2924209535121918\n",
            "Epoch 2, Batch 30, Loss: 0.29139411449432373\n",
            "Epoch 2, Batch 31, Loss: 0.2903235852718353\n",
            "Epoch 2, Batch 32, Loss: 0.2892673909664154\n",
            "Epoch 2, Batch 33, Loss: 0.28829750418663025\n",
            "Epoch 2, Batch 34, Loss: 0.28731849789619446\n",
            "Epoch 2, Batch 35, Loss: 0.28644439578056335\n",
            "Epoch 2, Batch 36, Loss: 0.28535255789756775\n",
            "Epoch 2, Batch 37, Loss: 0.2845171093940735\n",
            "Epoch 2, Batch 38, Loss: 0.2834356427192688\n",
            "Epoch 2, Batch 39, Loss: 0.28257158398628235\n",
            "Epoch 2, Batch 40, Loss: 0.2814842760562897\n",
            "Epoch 2, Batch 41, Loss: 0.28040745854377747\n",
            "Epoch 2, Batch 42, Loss: 0.2793256938457489\n",
            "Epoch 2, Batch 43, Loss: 0.27828580141067505\n",
            "Epoch 2, Batch 44, Loss: 0.27722617983818054\n",
            "Epoch 2, Batch 45, Loss: 0.2762746214866638\n",
            "Epoch 3, Batch 1, Loss: 0.27528107166290283\n",
            "Epoch 3, Batch 2, Loss: 0.2743421792984009\n",
            "Epoch 3, Batch 3, Loss: 0.2734244465827942\n",
            "Epoch 3, Batch 4, Loss: 0.2724090814590454\n",
            "Epoch 3, Batch 5, Loss: 0.27146458625793457\n",
            "Epoch 3, Batch 6, Loss: 0.270496666431427\n",
            "Epoch 3, Batch 7, Loss: 0.26942455768585205\n",
            "Epoch 3, Batch 8, Loss: 0.26893988251686096\n",
            "Epoch 3, Batch 9, Loss: 0.2681126594543457\n",
            "Epoch 3, Batch 10, Loss: 0.2670629322528839\n",
            "Epoch 3, Batch 11, Loss: 0.2660598158836365\n",
            "Epoch 3, Batch 12, Loss: 0.2651701867580414\n",
            "Epoch 3, Batch 13, Loss: 0.26425084471702576\n",
            "Epoch 3, Batch 14, Loss: 0.26345235109329224\n",
            "Epoch 3, Batch 15, Loss: 0.2623668909072876\n",
            "Epoch 3, Batch 16, Loss: 0.2613162696361542\n",
            "Epoch 3, Batch 17, Loss: 0.26040711998939514\n",
            "Epoch 3, Batch 18, Loss: 0.2596496343612671\n",
            "Epoch 3, Batch 19, Loss: 0.25867873430252075\n",
            "Epoch 3, Batch 20, Loss: 0.25766313076019287\n",
            "Epoch 3, Batch 21, Loss: 0.2567175328731537\n",
            "Epoch 3, Batch 22, Loss: 0.2557336688041687\n",
            "Epoch 3, Batch 23, Loss: 0.2548515796661377\n",
            "Epoch 3, Batch 24, Loss: 0.2539193630218506\n",
            "Epoch 3, Batch 25, Loss: 0.2530677318572998\n",
            "Epoch 3, Batch 26, Loss: 0.25220268964767456\n",
            "Epoch 3, Batch 27, Loss: 0.2513916790485382\n",
            "Epoch 3, Batch 28, Loss: 0.2505215108394623\n",
            "Epoch 3, Batch 29, Loss: 0.2497320920228958\n",
            "Epoch 3, Batch 30, Loss: 0.24897275865077972\n",
            "Epoch 3, Batch 31, Loss: 0.24810545146465302\n",
            "Epoch 3, Batch 32, Loss: 0.24719323217868805\n",
            "Epoch 3, Batch 33, Loss: 0.24631303548812866\n",
            "Epoch 3, Batch 34, Loss: 0.2453518956899643\n",
            "Epoch 3, Batch 35, Loss: 0.24455319344997406\n",
            "Epoch 3, Batch 36, Loss: 0.2437782734632492\n",
            "Epoch 3, Batch 37, Loss: 0.2428979128599167\n",
            "Epoch 3, Batch 38, Loss: 0.24206438660621643\n",
            "Epoch 3, Batch 39, Loss: 0.24131955206394196\n",
            "Epoch 3, Batch 40, Loss: 0.24053262174129486\n",
            "Epoch 3, Batch 41, Loss: 0.23974522948265076\n",
            "Epoch 3, Batch 42, Loss: 0.23885925114154816\n",
            "Epoch 3, Batch 43, Loss: 0.2380295842885971\n",
            "Epoch 3, Batch 44, Loss: 0.23726534843444824\n",
            "Epoch 3, Batch 45, Loss: 0.23651304841041565\n"
          ]
        }
      ],
      "source": [
        "def train_custom_autoencoder(custom_model, custom_data, epochs=10, batch_size=32):\n",
        "    for epoch in range(epochs):\n",
        "        np.random.shuffle(custom_data)\n",
        "        for i in range(0, len(custom_data), batch_size):\n",
        "            batch_custom_data = custom_data[i:i+batch_size]\n",
        "            if batch_custom_data.shape[0] != batch_size:\n",
        "                continue\n",
        "            loss_custom = custom_model.train_on_batch(batch_custom_data, batch_custom_data)\n",
        "            print(f\"Epoch {epoch + 1}, Batch {i // batch_size + 1}, Loss: {loss_custom}\")\n",
        "\n",
        "train_custom_autoencoder(autoencoder_custom, X_train_masked_custom, epochs=3, batch_size=5000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the Autoencoder**"
      ],
      "metadata": {
        "id": "V79SGmPaaMV7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtEFctdiXXXe",
        "outputId": "77b3a5a6-7b9c-4246-de0c-17d7bd18603f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 1, Discriminator Batch 1, Loss: 0.6853143572807312\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 1, Discriminator Batch 2, Loss: 0.6725725531578064\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 1, Discriminator Batch 3, Loss: 0.660690188407898\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 1, Discriminator Batch 4, Loss: 0.6495654582977295\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "Epoch 1, Discriminator Batch 5, Loss: 0.6388702988624573\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "Epoch 1, Discriminator Batch 6, Loss: 0.6288747191429138\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 1, Discriminator Batch 7, Loss: 0.6190589070320129\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 1, Discriminator Batch 8, Loss: 0.6096349954605103\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 1, Discriminator Batch 9, Loss: 0.6006031632423401\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 1, Discriminator Batch 10, Loss: 0.5919048190116882\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "Epoch 1, Discriminator Batch 11, Loss: 0.5837052464485168\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "Epoch 1, Discriminator Batch 12, Loss: 0.5756491422653198\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 1, Discriminator Batch 13, Loss: 0.5682206749916077\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 1, Discriminator Batch 14, Loss: 0.5610604882240295\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 1, Discriminator Batch 15, Loss: 0.5541881918907166\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "Epoch 1, Discriminator Batch 16, Loss: 0.5475073456764221\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 1, Discriminator Batch 17, Loss: 0.5411303043365479\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 1, Discriminator Batch 18, Loss: 0.5350421667098999\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 1, Discriminator Batch 19, Loss: 0.5291156768798828\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 1, Discriminator Batch 20, Loss: 0.5234734416007996\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 1, Discriminator Batch 21, Loss: 0.5180321335792542\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 1, Discriminator Batch 22, Loss: 0.5128045082092285\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 1, Discriminator Batch 23, Loss: 0.5077439546585083\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 1, Discriminator Batch 24, Loss: 0.5028968453407288\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 1, Discriminator Batch 25, Loss: 0.4982280135154724\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 1, Discriminator Batch 26, Loss: 0.49370187520980835\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 1, Discriminator Batch 27, Loss: 0.4892893433570862\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 1, Discriminator Batch 28, Loss: 0.48506730794906616\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 1, Discriminator Batch 29, Loss: 0.48095113039016724\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 1, Discriminator Batch 30, Loss: 0.47693145275115967\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "Epoch 1, Discriminator Batch 31, Loss: 0.473018079996109\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 1, Discriminator Batch 32, Loss: 0.4692237377166748\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 1, Discriminator Batch 33, Loss: 0.4655582308769226\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 1, Discriminator Batch 34, Loss: 0.4619911313056946\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "Epoch 1, Discriminator Batch 35, Loss: 0.4585282504558563\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 1, Discriminator Batch 36, Loss: 0.4551401138305664\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 1, Discriminator Batch 37, Loss: 0.4518537223339081\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 1, Discriminator Batch 38, Loss: 0.4486278295516968\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 1, Discriminator Batch 39, Loss: 0.4454830586910248\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 1, Discriminator Batch 40, Loss: 0.44240933656692505\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 1, Discriminator Batch 41, Loss: 0.43941885232925415\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 1, Discriminator Batch 42, Loss: 0.4364859461784363\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 1, Discriminator Batch 43, Loss: 0.4336126148700714\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "Epoch 1, Discriminator Batch 44, Loss: 0.4308087229728699\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 1, Discriminator Batch 45, Loss: 0.428048312664032\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "Epoch 2, Discriminator Batch 1, Loss: 0.42534035444259644\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 2, Discriminator Batch 2, Loss: 0.4226783812046051\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 2, Discriminator Batch 3, Loss: 0.42007169127464294\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 2, Discriminator Batch 4, Loss: 0.417507529258728\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 2, Discriminator Batch 5, Loss: 0.41498616337776184\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 2, Discriminator Batch 6, Loss: 0.4124986231327057\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 2, Discriminator Batch 7, Loss: 0.4100377559661865\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 2, Discriminator Batch 8, Loss: 0.4076235294342041\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 2, Discriminator Batch 9, Loss: 0.40523865818977356\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 2, Discriminator Batch 10, Loss: 0.40287911891937256\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 2, Discriminator Batch 11, Loss: 0.40055006742477417\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "Epoch 2, Discriminator Batch 12, Loss: 0.3982471227645874\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 2, Discriminator Batch 13, Loss: 0.39596256613731384\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "Epoch 2, Discriminator Batch 14, Loss: 0.3937051296234131\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 2, Discriminator Batch 15, Loss: 0.39146745204925537\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 2, Discriminator Batch 16, Loss: 0.38924381136894226\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 2, Discriminator Batch 17, Loss: 0.38704848289489746\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 2, Discriminator Batch 18, Loss: 0.3848610520362854\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 2, Discriminator Batch 19, Loss: 0.38268858194351196\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 2, Discriminator Batch 20, Loss: 0.3805336654186249\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 2, Discriminator Batch 21, Loss: 0.37838849425315857\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 2, Discriminator Batch 22, Loss: 0.3762519061565399\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 2, Discriminator Batch 23, Loss: 0.37412944436073303\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 2, Discriminator Batch 24, Loss: 0.37200576066970825\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 2, Discriminator Batch 25, Loss: 0.3698936402797699\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 2, Discriminator Batch 26, Loss: 0.36779341101646423\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 2, Discriminator Batch 27, Loss: 0.365694522857666\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 2, Discriminator Batch 28, Loss: 0.36360329389572144\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 2, Discriminator Batch 29, Loss: 0.3615134358406067\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 2, Discriminator Batch 30, Loss: 0.3594367206096649\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 2, Discriminator Batch 31, Loss: 0.35736188292503357\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 2, Discriminator Batch 32, Loss: 0.35528698563575745\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 2, Discriminator Batch 33, Loss: 0.35321545600891113\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 2, Discriminator Batch 34, Loss: 0.35114967823028564\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 2, Discriminator Batch 35, Loss: 0.3490864038467407\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "Epoch 2, Discriminator Batch 36, Loss: 0.34703338146209717\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 2, Discriminator Batch 37, Loss: 0.34497570991516113\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 2, Discriminator Batch 38, Loss: 0.34291964769363403\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 2, Discriminator Batch 39, Loss: 0.340859979391098\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 2, Discriminator Batch 40, Loss: 0.3388059139251709\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 2, Discriminator Batch 41, Loss: 0.33675453066825867\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 2, Discriminator Batch 42, Loss: 0.33471372723579407\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 2, Discriminator Batch 43, Loss: 0.3326728641986847\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 2, Discriminator Batch 44, Loss: 0.33063897490501404\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 2, Discriminator Batch 45, Loss: 0.3285975754261017\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 3, Discriminator Batch 1, Loss: 0.3265590965747833\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 3, Discriminator Batch 2, Loss: 0.3245258033275604\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 3, Discriminator Batch 3, Loss: 0.32249748706817627\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 3, Discriminator Batch 4, Loss: 0.32047635316848755\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 3, Discriminator Batch 5, Loss: 0.3184541165828705\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 3, Discriminator Batch 6, Loss: 0.3164409101009369\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "Epoch 3, Discriminator Batch 7, Loss: 0.31443241238594055\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "Epoch 3, Discriminator Batch 8, Loss: 0.31243300437927246\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 3, Discriminator Batch 9, Loss: 0.31044596433639526\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 3, Discriminator Batch 10, Loss: 0.30845752358436584\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "Epoch 3, Discriminator Batch 11, Loss: 0.3064727783203125\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 3, Discriminator Batch 12, Loss: 0.3045063614845276\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 3, Discriminator Batch 13, Loss: 0.30253922939300537\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 3, Discriminator Batch 14, Loss: 0.30058175325393677\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "Epoch 3, Discriminator Batch 15, Loss: 0.298636794090271\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 3, Discriminator Batch 16, Loss: 0.2966994643211365\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 3, Discriminator Batch 17, Loss: 0.2947700023651123\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 3, Discriminator Batch 18, Loss: 0.2928498685359955\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 3, Discriminator Batch 19, Loss: 0.2909487783908844\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 3, Discriminator Batch 20, Loss: 0.2890505790710449\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 3, Discriminator Batch 21, Loss: 0.2871691882610321\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 3, Discriminator Batch 22, Loss: 0.28529825806617737\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 3, Discriminator Batch 23, Loss: 0.28343644738197327\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 3, Discriminator Batch 24, Loss: 0.28158360719680786\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 3, Discriminator Batch 25, Loss: 0.2797415256500244\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 3, Discriminator Batch 26, Loss: 0.2779183089733124\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "Epoch 3, Discriminator Batch 27, Loss: 0.27611368894577026\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 3, Discriminator Batch 28, Loss: 0.2743126451969147\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "Epoch 3, Discriminator Batch 29, Loss: 0.27252888679504395\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 3, Discriminator Batch 30, Loss: 0.27076277136802673\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "Epoch 3, Discriminator Batch 31, Loss: 0.26900583505630493\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "Epoch 3, Discriminator Batch 32, Loss: 0.2672654688358307\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 3, Discriminator Batch 33, Loss: 0.265544056892395\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "Epoch 3, Discriminator Batch 34, Loss: 0.26383382081985474\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "Epoch 3, Discriminator Batch 35, Loss: 0.262143075466156\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "Epoch 3, Discriminator Batch 36, Loss: 0.26046445965766907\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 3, Discriminator Batch 37, Loss: 0.2587999701499939\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "Epoch 3, Discriminator Batch 38, Loss: 0.2571451663970947\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "Epoch 3, Discriminator Batch 39, Loss: 0.25550997257232666\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Epoch 3, Discriminator Batch 40, Loss: 0.253887414932251\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 3, Discriminator Batch 41, Loss: 0.25227972865104675\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "Epoch 3, Discriminator Batch 42, Loss: 0.25068554282188416\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 3, Discriminator Batch 43, Loss: 0.24910986423492432\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Epoch 3, Discriminator Batch 44, Loss: 0.24754692614078522\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "Epoch 3, Discriminator Batch 45, Loss: 0.24599948525428772\n"
          ]
        }
      ],
      "source": [
        "def train_custom_discriminator(custom_discriminator, custom_autoencoder, custom_real_data, epochs=10, batch_size=32):\n",
        "    for epoch in range(epochs):\n",
        "        np.random.shuffle(custom_real_data)\n",
        "        for i in range(0, len(custom_real_data), batch_size):\n",
        "            custom_real_batch = custom_real_data[i:i+batch_size]\n",
        "            if custom_real_batch.shape[0] != batch_size:\n",
        "                continue\n",
        "            custom_fake_batch = custom_autoencoder.predict(custom_real_batch)\n",
        "            custom_x_combined_batch = np.concatenate((custom_real_batch, custom_fake_batch))\n",
        "            custom_y_combined_batch = np.concatenate((np.ones((batch_size, 1)), np.zeros((batch_size, 1))))\n",
        "            loss_custom = custom_discriminator.train_on_batch(custom_x_combined_batch, custom_y_combined_batch)\n",
        "            print(f\"Epoch {epoch + 1}, Discriminator Batch {i // batch_size + 1}, Loss: {loss_custom}\")\n",
        "\n",
        "train_custom_discriminator(discriminator_custom, autoencoder_custom, X_train_credit.values, epochs=3, batch_size=5000)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the Discriminator**"
      ],
      "metadata": {
        "id": "HWG6U3VQaTTQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUxx-bV2XXXe",
        "outputId": "88cbf418-8380-4871-9241-8845fef5c304"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step\n"
          ]
        }
      ],
      "source": [
        "def detect_custom_anomalies(custom_model, custom_data):\n",
        "    custom_predictions = custom_model.predict(custom_data)\n",
        "    custom_mse = np.mean(np.power(custom_data - custom_predictions, 2), axis=1)\n",
        "    return custom_mse\n",
        "\n",
        "anomaly_scores_custom = detect_custom_anomalies(autoencoder_custom, X_test_credit.values)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Detecting Anomalies and Setting Thresholds**"
      ],
      "metadata": {
        "id": "yJ0oczLKZwSA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPkJ8PvCXXXf",
        "outputId": "ac1b5193-ff7e-4bb4-ed2d-ded04245bf78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE Threshold:  8.362404027002002\n",
            "Anomaly Detected:\n",
            "               V1         V2         V3        V4         V5         V6  \\\n",
            "43428  -16.526507   8.584972 -18.649853  9.505594 -13.793819  -2.832404   \n",
            "219257 -29.942972 -25.831782 -16.227512  6.690679 -20.787846  13.085694   \n",
            "113740  -5.136552   5.746647  -3.838599 -0.329163   1.288327   0.251632   \n",
            "154372  -8.339783   7.278206  -6.017969 -2.170078  -2.006120  -1.717894   \n",
            "172250  -5.013928  -3.169697  -3.283770  1.125001  -9.982772   7.355276   \n",
            "...           ...        ...        ...       ...        ...        ...   \n",
            "123326 -10.924306   8.486126  -5.303020 -2.547358  -5.055657  -1.375718   \n",
            "92366   -8.254743   6.837986  -6.780968  0.670752  -4.482530  -1.176162   \n",
            "33756   -7.504323 -15.633772  -4.215051  1.507090  -5.412604   3.243205   \n",
            "44614   -9.482402   5.448324  -7.587924  1.334433  -5.916629  -1.842644   \n",
            "248050  -2.897780  -9.166194  -5.001077  0.732565  -1.804056   1.973025   \n",
            "\n",
            "               V7        V8        V9        V10  ...       V21       V22  \\\n",
            "43428  -16.701694  7.517344 -8.507059 -14.110184  ...  1.190739 -1.127670   \n",
            "219257  17.256623 -9.161746  5.003041  -2.431466  ... -2.494699 -0.660297   \n",
            "113740   1.442205 -0.379463  5.509988   8.457899  ... -1.291890  0.247433   \n",
            "154372  -1.266732  2.951894  4.446884   5.759475  ... -0.354493  0.133329   \n",
            "172250  11.858880 -2.399753 -0.041932  -1.397333  ... -2.062718  0.326523   \n",
            "...           ...       ...       ...        ...  ...       ...       ...   \n",
            "123326  -3.642639  5.227469  3.425662   5.856304  ... -0.423916 -0.701414   \n",
            "92366   -4.629440  6.416157 -0.844921   0.997212  ...  0.466668  0.384127   \n",
            "33756    4.422388 -0.423582  0.392031  -2.580709  ...  2.323590 -2.539173   \n",
            "44614   -3.627962  6.078775 -0.245652   1.010897  ...  0.220940 -0.172679   \n",
            "248050   2.926021 -0.434329 -1.100017  -0.309766  ...  1.931108 -0.290744   \n",
            "\n",
            "             V23       V24       V25       V26       V27       V28     Amount  \\\n",
            "43428  -2.358579  0.673461 -1.413700 -0.462762 -2.018575 -1.042804   1.102834   \n",
            "219257 -8.537816  0.400804 -0.643023  0.496903  6.267709 -2.765070  13.648508   \n",
            "113740  0.182054 -1.486916  0.106656 -0.649515 -0.708468 -2.730412  -0.350071   \n",
            "154372  0.418412  0.580889  1.129070  0.130860  2.153293  1.554899  -0.318126   \n",
            "172250  1.604230  0.530301  0.506055 -0.722914  2.383381 -0.664019   9.641986   \n",
            "...          ...       ...       ...       ...       ...       ...        ...   \n",
            "123326  0.918638  0.126006  1.280962  0.881748  1.971578  1.240784  -0.201982   \n",
            "92366   0.971214 -0.329410  0.202777 -0.326274 -0.478380 -0.107683  -0.346073   \n",
            "33756  -3.736852 -0.792271 -1.694790 -0.463947 -0.703180  0.780931  17.545083   \n",
            "44614  -0.249552  0.426950  0.091858 -0.373437  0.149692 -0.076027   0.006558   \n",
            "248050 -2.240894 -0.774535 -0.868359 -0.217680 -0.505657  0.360700  10.096968   \n",
            "\n",
            "        Class  \n",
            "43428       1  \n",
            "219257      0  \n",
            "113740      0  \n",
            "154372      0  \n",
            "172250      0  \n",
            "...       ...  \n",
            "123326      0  \n",
            "92366       0  \n",
            "33756       0  \n",
            "44614       0  \n",
            "248050      0  \n",
            "\n",
            "[621 rows x 30 columns]\n",
            "Number of anomalies detected: 621\n"
          ]
        }
      ],
      "source": [
        "# Calculate a threshold\n",
        "custom_mse_threshold = np.mean(anomaly_scores_custom) + 2 * np.std(anomaly_scores_custom)\n",
        "print(\"Custom MSE Threshold: \", custom_mse_threshold)\n",
        "\n",
        "# Identify anomalies\n",
        "custom_anomalies = anomaly_scores_custom > custom_mse_threshold\n",
        "\n",
        "# Show anomalous data\n",
        "print(\"Anomaly Detected:\")\n",
        "print(X_test_credit[custom_anomalies])\n",
        "\n",
        "# Optional: Count of anomalies\n",
        "print(\"Number of anomalies detected:\", np.sum(custom_anomalies))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}